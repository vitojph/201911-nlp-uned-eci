{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectores de Palabras\n",
    "\n",
    "La idea de que dos palabras pueden ser más o menos similares es crucial para muchas tareas de NLP.\n",
    "\n",
    "Esta idea de similitud es más fácil de manejar si representamos las palabras como un vectores en lugar de enteros. Podemos decidir qué rasgos/características tenemos en cuenta, y cuántas dimensiones tienen nuestros vectores. Podemos diseñarlos a mano. Podemos calcularlos automáticamente.\n",
    "\n",
    "En lingüística, en Semántica Distribucional, está claro que si dos palabras pueden utilizarse de manera similar es porque tienen significados parecidos.\n",
    "\n",
    "> *You shall know a word by the company it keeps.* John R. Firth\n",
    "\n",
    "> *The meaning of a word is its use in the language (…) One cannot guess how a word functions. One has to look at its use, and learn from that.* L. Wittgenstein \n",
    "\n",
    "Dos palabras son similares si aparecen en contextos similares.\n",
    "\n",
    "\n",
    "Los modelos de espacio vectorial ([*vector space models*](https://en.wikipedia.org/wiki/Vector_space_model)) permiten representar palabras o términos dentro de un espacio vectorial continuo, de manera que las palabras que son similares desde el punto semántico se situan en puntos cercanos dentro de ese espacio común.\n",
    "\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png)\n",
    "\n",
    "\n",
    "## `word2vec`\n",
    "\n",
    "(Mikolov, 2013) describe [word2vec](https://en.wikipedia.org/wiki/Word2vec), un algoritmo no supervisado para calcular el significado de palabras como representaciones vectoriales densas, como listas de números reales.\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/projects/glove/), en 2014, propone un mecanismo similar.\n",
    "\n",
    "Estos sistemas aprenden el significado de las palabras, y parecen capturar cierta dirección semántica y morfo-sintáctica.\n",
    "\n",
    "    París - France + España ≈ Madrid\n",
    "    comimos - comer + andar ≈ anduvimos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud semántica entre palabras, frases y documentos\n",
    "\n",
    "spaCy permite [calcular la similitud semántica](https://spacy.io/usage/vectors-similarity) entre cualquier par de objetos de tipo `Doc`, `Span` o `Token`. \n",
    "\n",
    "Ojo, La similitud semántica es un concepto algo subjetivo, pero en este caso se puede entender como la probabilidad de que dos palabras aparezcan en los mismos contextos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "nlp_es = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analizamos algunas colocaciones en inglés\n",
    "token1, _, token2 = nlp_en(\"cats and dogs\")\n",
    "token3, _, token4 = nlp_en(\"research and development\")\n",
    "\n",
    "# medimos la similitud semántica entre algunos pares\n",
    "print(f\"{token1} vs {token2}: {token1.similarity(token2)}\")\n",
    "print(f\"{token3} vs {token4}: {token3.similarity(token4)}\")\n",
    "print(f\"{token1} vs {token4}: {token1.similarity(token4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿qué tal funciona en español?\n",
    "token1, _, token2 = nlp_es(\"perros y gatos\")\n",
    "token3, _, token4 = nlp_es(\"investigación y desarrollo\")\n",
    "\n",
    "# medimos la similitud semántica entre algunos pares\n",
    "print(f\"{token1} vs {token2}: {token1.similarity(token2)}\")\n",
    "print(f\"{token3} vs {token4}: {token3.similarity(token4)}\")\n",
    "print(f\"{token1} vs {token4}: {token1.similarity(token4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{token1} tiene {token1.vector.shape} dimensiones\")\n",
    "print(token1.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de `word2vec` con `gensim`\n",
    "\n",
    "En la siguiente celda, importamos las librerías necesarias y configuramos los mensajes de los logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, os\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "model = gensim.models.Word2Vec.load(\"/home/victor/w2v/eswiki-300.w2v\")\n",
    "\n",
    "print(f\"Modelo cargado con un vocabulario de {model.corpus_count} términos diferentes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada término del vocabulario está representado como un vector con 300 dimensiones. Podemos acceder al vector de un término concreto, aunque probablemente no nos diga mucho, salvo que contienen números muy pequeños :-/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[\"azul\"])\n",
    "print(model[\"verde\"])\n",
    "print(model[\"clorofila\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este mismo objeto `model` permite acceder a una serie de funcionalidades ya implementadas que nos van a permitir evaluar formal e informalmente el modelo. Por el momento, nos contentamos con los segundo: vamos a revisar visualmente los significados que nuestro modelo ha aprendido por su cuenta. \n",
    "\n",
    "Podemos calcular la similitud semántica entre dos términos usando el método `similarity`, que nos devuelve un número entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hombre - mujer\", model.wv.similarity(\"hombre\", \"mujer\"))\n",
    "\n",
    "print(\"perro - gato\", model.wv.similarity(\"perro\", \"gato\"))\n",
    "\n",
    "print(\"gato - periódico\", model.wv.similarity(\"gato\", \"periódico\"))\n",
    "\n",
    "print(\"febrero - azul\", model.wv.similarity(\"febrero\", \"azul\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos seleccionar el término que no encaja a partir de una determinada lista de términos usando el método `doesnt_match`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1 = \"madrid barcelona gonzález washington\".split()\n",
    "print(\"en la lista\", \" \".join(lista1), \"sobra:\", model.wv.doesnt_match(lista1))\n",
    "\n",
    "lista2 = \"psoe pp ciu ronaldo\".split()\n",
    "print(\"en la lista\", \" \".join(lista2), \"sobra:\", model.wv.doesnt_match(lista2))\n",
    "\n",
    "lista3 = \"publicaron declararon soy negaron\".split()\n",
    "print(\"en la lista\", \" \".join(lista3), \"sobra:\", model.wv.doesnt_match(lista3))\n",
    "\n",
    "lista4 = \"homero saturno cervantes shakespeare cela\".split()\n",
    "print(\"en la lista\", \" \".join(lista4), \"sobra:\", model.wv.doesnt_match(lista4))\n",
    "\n",
    "lista5 = \"madrid barcelona alpedrete marsella\".split()\n",
    "print(\"en la lista\", \" \".join(lista5), \"sobra:\", model.wv.doesnt_match(lista5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos buscar los términos más similares usando el método `most_similar` de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos = \"azul madrid bmw bici 2018 rock google psoe jay-z xiaomi rajoy brexit saturno césar lazio\".split()\n",
    "\n",
    "for t in terminos:\n",
    "    print(t, \"==>\", model.wv.most_similar(t), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el mismo método `most_similar` podemos combinar vectores de palabras tratando de jugar con los rasgos semánticos de cada una de ellas para descubrir nuevas relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mujer que ejerce la autoridad en una alcaldía ==> alcalde + mujer - hombre\")\n",
    "most_similar = model.wv.most_similar(positive=[\"alcalde\", \"mujer\"], negative=[\"hombre\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "    \n",
    "print(\"mujer especializada en alguna terapia de la medicina ==> doctor + mujer - hombre\")\n",
    "most_similar = model.wv.most_similar(positive=[\"doctor\", \"mujer\"], negative=[\"hombre\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print(\"monarca soberano ==> reina + hombre - mujer\")    \n",
    "most_similar = model.wv.most_similar(positive=[\"reina\", \"hombre\"], negative=[\"mujer\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "    \n",
    "print(\"capital de Alemania ==> moscú + alemania - rusia\")\n",
    "most_similar = model.wv.most_similar(positive=[\"moscú\", \"alemania\"], negative=[\"rusia\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print(\"presidente de Francia ==> obama + francia - eeuu\")\n",
    "most_similar = model.wv.most_similar(positive=[\"obama\", \"francia\"], negative=[\"eeuu\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fastText`\n",
    "\n",
    "`word2vec` y `GloVe` tienen un problema: no saben cómo manejar palabras desconocidas (*OOV words*).\n",
    "\n",
    "`fastText` (Bojanowski et al., 2016) describe un método para aprender vectores, no de palabras, sino de n-gramas o secuencias de caracteres (*subword information*).\n",
    "\n",
    "Es capaz de manejar erratas y palabras raras y[está disponible en 157 lenguas del mundo](https://fasttext.cc/docs/en/crawl-vectors.html#models).\n",
    "\n",
    "Pero, estos modelos tienen una limitación importante: cada palabra se codifica como un único vector.\n",
    "\n",
    "Y el lenguaje natural es ambiguo, a varios niveles. De hecho, existen fenómenos como la polisemia y la homografía.\n",
    "\n",
    "- *banco*: de sentar, de crédito, de peces, de arena\n",
    "- *cubo*: de agua, operación matemática, figura geométrica\n",
    "- *planta*: arbusto, fábrica, del pie\n",
    "- *bajo*: nombre, adjetivo, preposición\n",
    "\n",
    "## Barrio Sésamo al Rescate\n",
    "\n",
    "![](http://2kceni4cuv0n2yx0s3ezy0bk-wpengine.netdna-ssl.com/wp-content/uploads/2019/07/Lb8cuv8dXPRRA_Cnay4unXbYz4nRc1Vn8ubwuvyki5Citk22zbg4bfjfRzJmlIHSrwhJmpZWGqiSdEQ8mP0xyw4ZXBoWoLdFAQ-HChX05RZONDpo6xoeuKWh8i_choOfk9l7Sd2y.png)\n",
    "\n",
    "## ELMo\n",
    "\n",
    "ELMo (Peters et al., 2018) es una nueva técnica para calcular y asignar vectores de palabras de manera dinámica.\n",
    "\n",
    "Los vectores que asigna ELMo a las palabras cambian dependiendo del contexto porque tiene en cuenta todo el texto.\n",
    "\n",
    "El algoritmo combina dos modelos de lenguaje para calcular probabilidades de una palabra considerando su contexto anterior y posterior.\n",
    "\n",
    "## BERT\n",
    "\n",
    "BERT (Devlin et al., 2018) es otro algoritmo para codificar texto como vectores dinámicos.\n",
    "\n",
    "Combina varias ideas de ELMo (modelos de lenguaje bidireccionales) y otras arquitecturas de DL ([transformers](http://www.peterbloem.nl/blog/transformers)) para crear un modelo generalista que funciona sorprendentemente bien en cualquier tarea y es muy fácil de ajustar para tareas concretas.\n",
    "\n",
    "### Pero es que hay más\n",
    "\n",
    "Por fin contamos con modelos de lenguaje pre-entrenados que pueden ser ajustados (*fine tuned*) para tareas concretas.\n",
    "\n",
    "- [ULMFiT](http://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.html)\n",
    "- [OpenAI GPT-2](https://openai.com/blog/better-language-models/)\n",
    "- [XLNet](https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335)\n",
    "- [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)\n",
    "- [ALBERT](https://medium.com/@lessw/meet-albert-a-new-lite-bert-from-google-toyota-with-state-of-the-art-nlp-performance-and-18x-df8f7b58fa28)\n",
    "\n",
    "\n",
    "# Modelos de Lenguaje de OpenAI\n",
    "\n",
    "A mitad de febrero, [OpenAI publicó un modelo de lenguaje](https://blog.openai.com/better-language-models/) capaz de generar lenguaje natural de formar coherente. Este modelo es generalista y, a pesar de ello, es capaz de rivalizar con los mejores sistemas específicos en tareas como comprensión automática de lenguaje natural, traducción automática, búsqueda de respuestas y resumen automático.\n",
    "\n",
    "Este modelo, llamado GPT-2, es el resultado de haber entrenado con 8 millones de páginas web (40 GB) con 1 500 millones de parámetros con un único objetivo: predecir cuál es la siguiente palabra.\n",
    "\n",
    "Sin embargo, OpenAI no ha publicado [hasta justo esta semana](https://twitter.com/OpenAI/status/1191764001434173440) el modelo más grande y potente que tenían, para evitar que alguien con malas intenciones pueda hacer un uso dañino de esta tecnología. Sí que han publicado una versión simplificada y más pequeña, y el paper [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), en el que explican todo el proceso.\n",
    "\n",
    "Con ganas y GPUs suficientes (+ tiempo y dinero), se puede replicar el proceso. Otras lecturas interesantes, sobre el tema: \n",
    "\n",
    "- [OpenAI's new Multitalented AI Writes, Translates, and Slanders](https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2)\n",
    "- [Some thoughts on zero-day threats in AI, and OpenAI's GP2](https://www.fast.ai/2019/02/15/openai-gp2/)\n",
    "\n",
    "\n",
    "Este código de ejemplo está inspirado en [un tweet de Thomas Wolf](https://twitter.com/Thom_Wolf/status/1097465312579072000), de [Hugging Face](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F \n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, definimos una función para:\n",
    "\n",
    "1. tokenizar el texto de entrada y codificarlo como un vector con los pesos obtenidos por el modelo GPT2\n",
    "2. predecir la siguiente palabra más frecuente\n",
    "3. decodificar el vector como una secuencia de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, length=50):\n",
    "    \"\"\"Generate automatic Natural Language from the input text\"\"\"\n",
    "    vec_text = tokenizer.encode(text)\n",
    "    my_input, past = torch.tensor([vec_text]), None\n",
    "    \n",
    "    for _ in range(length):\n",
    "        logits, past = model(my_input, past=past)\n",
    "        my_input = torch.multinomial(F.softmax(logits[:, -1]), 1)\n",
    "        vec_text.append(my_input.item())\n",
    "    \n",
    "    return tokenizer.decode(vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defino un texto de entrada\n",
    "text = \"The only think we can do to fight climate change is\"\n",
    "\n",
    "# y generamos automáticamente las secuencias más probables\n",
    "print(generate(text, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
