{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectores de Palabras\n",
    "\n",
    "La idea de que dos palabras pueden ser más o menos similares es crucial para muchas tareas de NLP.\n",
    "\n",
    "Esta idea de similitud es más fácil de manejar si representamos las palabras como un vectores en lugar de enteros. Podemos decidir qué rasgos/características tenemos en cuenta, y cuántas dimensiones tienen nuestros vectores. Podemos diseñarlos a mano. Podemos calcularlos automáticamente.\n",
    "\n",
    "En lingüística, en Semántica Distribucional, está claro que si dos palabras pueden utilizarse de manera similar es porque tienen significados parecidos.\n",
    "\n",
    "> *You shall know a word by the company it keeps.* John R. Firth\n",
    "\n",
    "> *The meaning of a word is its use in the language (…) One cannot guess how a word functions. One has to look at its use, and learn from that.* L. Wittgenstein \n",
    "\n",
    "Dos palabras son similares si aparecen en contextos similares.\n",
    "\n",
    "\n",
    "Los modelos de espacio vectorial ([*vector space models*](https://en.wikipedia.org/wiki/Vector_space_model)) permiten representar palabras o términos dentro de un espacio vectorial continuo, de manera que las palabras que son similares desde el punto semántico se situan en puntos cercanos dentro de ese espacio común.\n",
    "\n",
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similitud semántica entre palabras, frases y documentos\n",
    "\n",
    "spaCy permite [calcular la similitud semántica](https://spacy.io/usage/vectors-similarity) entre cualquier par de objetos de tipo `Doc`, `Span` o `Token`. \n",
    "\n",
    "Ojo, La similitud semántica es un concepto algo subjetivo, pero en este caso se puede entender como la probabilidad de que dos palabras aparezcan en los mismos contextos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "nlp_es = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analizamos algunas colocaciones en inglés\n",
    "token1, _, token2 = nlp_en(\"cats and dogs\")\n",
    "token3, _, token4 = nlp_en(\"research and development\")\n",
    "\n",
    "# medimos la similitud semántica entre algunos pares\n",
    "print(f\"{token1} vs {token2}: {token1.similarity(token2)}\")\n",
    "print(f\"{token3} vs {token4}: {token3.similarity(token4)}\")\n",
    "print(f\"{token1} vs {token4}: {token1.similarity(token4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿qué tal funciona en español?\n",
    "token1, _, token2 = nlp_es(\"perros y gatos\")\n",
    "token3, _, token4 = nlp_es(\"investigación y desarrollo\")\n",
    "\n",
    "# medimos la similitud semántica entre algunos pares\n",
    "print(f\"{token1} vs {token2}: {token1.similarity(token2)}\")\n",
    "print(f\"{token3} vs {token4}: {token3.similarity(token4)}\")\n",
    "print(f\"{token1} vs {token4}: {token1.similarity(token4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{token1} tiene {token1.vector.shape} dimensiones\")\n",
    "print(token1.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de `word2vec` con `gensim`\n",
    "\n",
    "En la siguiente celda, importamos las librerías necesarias y configuramos los mensajes de los logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, os\n",
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)\n",
    "\n",
    "model = gensim.models.Word2Vec.load(\"/home/victor/w2v/eswiki-300.w2v\")\n",
    "\n",
    "print(f\"Modelo cargado con un vocabulario de {model.corpus_count} términos diferentes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada término del vocabulario está representado como un vector con 300 dimensiones. Podemos acceder al vector de un término concreto, aunque probablemente no nos diga mucho, salvo que contienen números muy pequeños :-/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model[\"azul\"])\n",
    "print(model[\"verde\"])\n",
    "print(model[\"clorofila\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este mismo objeto `model` permite acceder a una serie de funcionalidades ya implementadas que nos van a permitir evaluar formal e informalmente el modelo. Por el momento, nos contentamos con los segundo: vamos a revisar visualmente los significados que nuestro modelo ha aprendido por su cuenta. \n",
    "\n",
    "Podemos calcular la similitud semántica entre dos términos usando el método `similarity`, que nos devuelve un número entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hombre - mujer\", model.wv.similarity(\"hombre\", \"mujer\"))\n",
    "\n",
    "print(\"perro - gato\", model.wv.similarity(\"perro\", \"gato\"))\n",
    "\n",
    "print(\"gato - periódico\", model.wv.similarity(\"gato\", \"periódico\"))\n",
    "\n",
    "print(\"febrero - azul\", model.wv.similarity(\"febrero\", \"azul\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos seleccionar el término que no encaja a partir de una determinada lista de términos usando el método `doesnt_match`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista1 = \"madrid barcelona gonzález washington\".split()\n",
    "print(\"en la lista\", \" \".join(lista1), \"sobra:\", model.wv.doesnt_match(lista1))\n",
    "\n",
    "lista2 = \"psoe pp ciu ronaldo\".split()\n",
    "print(\"en la lista\", \" \".join(lista2), \"sobra:\", model.wv.doesnt_match(lista2))\n",
    "\n",
    "lista3 = \"publicaron declararon soy negaron\".split()\n",
    "print(\"en la lista\", \" \".join(lista3), \"sobra:\", model.wv.doesnt_match(lista3))\n",
    "\n",
    "lista4 = \"homero saturno cervantes shakespeare cela\".split()\n",
    "print(\"en la lista\", \" \".join(lista4), \"sobra:\", model.wv.doesnt_match(lista4))\n",
    "\n",
    "lista5 = \"madrid barcelona alpedrete marsella\".split()\n",
    "print(\"en la lista\", \" \".join(lista5), \"sobra:\", model.wv.doesnt_match(lista5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos buscar los términos más similares usando el método `most_similar` de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminos = \"azul madrid bmw bici 2018 rock google psoe jay-z xiaomi rajoy brexit saturno césar lazio\".split()\n",
    "\n",
    "for t in terminos:\n",
    "    print(t, \"==>\", model.wv.most_similar(t), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el mismo método `most_similar` podemos combinar vectores de palabras tratando de jugar con los rasgos semánticos de cada una de ellas para descubrir nuevas relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mujer que ejerce la autoridad en una alcaldía ==> alcalde + mujer - hombre\")\n",
    "most_similar = model.wv.most_similar(positive=[\"alcalde\", \"mujer\"], negative=[\"hombre\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "    \n",
    "print(\"mujer especializada en alguna terapia de la medicina ==> doctor + mujer - hombre\")\n",
    "most_similar = model.wv.most_similar(positive=[\"doctor\", \"mujer\"], negative=[\"hombre\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print(\"monarca soberano ==> reina + hombre - mujer\")    \n",
    "most_similar = model.wv.most_similar(positive=[\"reina\", \"hombre\"], negative=[\"mujer\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "    \n",
    "print(\"capital de Alemania ==> moscú + alemania - rusia\")\n",
    "most_similar = model.wv.most_similar(positive=[\"moscú\", \"alemania\"], negative=[\"rusia\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)\n",
    "\n",
    "print(\"presidente de Francia ==> obama + francia - eeuu\")\n",
    "most_similar = model.wv.most_similar(positive=[\"obama\", \"francia\"], negative=[\"eeuu\"], topn=3)\n",
    "for item in most_similar:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Lenguaje de OpenAI\n",
    "\n",
    "A mitad de febrero, [OpenAI publicó un modelo de lenguaje](https://blog.openai.com/better-language-models/) capaz de generar lenguaje natural de formar coherente. Este modelo es generalista y, a pesar de ello, es capaz de rivalizar con los mejores sistemas específicos en tareas como comprensión automática de lenguaje natural, traducción automática, búsqueda de respuestas y resumen automático.\n",
    "\n",
    "Este modelo, llamado GPT-2, es el resultado de haber entrenado con 8 millones de páginas web (40 GB) con 1 500 millones de parámetros con un único objetivo: predecir cuál es la siguiente palabra.\n",
    "\n",
    "Sin embargo, OpenAI no ha publicado [hasta justo esta semana](https://twitter.com/OpenAI/status/1191764001434173440) el modelo más grande y potente que tenían, para evitar que alguien con malas intenciones pueda hacer un uso dañino de esta tecnología. Sí que han publicado una versión simplificada y más pequeña, y el paper [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), en el que explican todo el proceso.\n",
    "\n",
    "Con ganas y GPUs suficientes (+ tiempo y dinero), se puede replicar el proceso. Otras lecturas interesantes, sobre el tema: \n",
    "\n",
    "- [OpenAI's new Multitalented AI Writes, Translates, and Slanders](https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2)\n",
    "- [Some thoughts on zero-day threats in AI, and OpenAI's GP2](https://www.fast.ai/2019/02/15/openai-gp2/)\n",
    "\n",
    "\n",
    "Este código de ejemplo está inspirado en [un tweet de Thomas Wolf](https://twitter.com/Thom_Wolf/status/1097465312579072000), de [Hugging Face](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import functional as F \n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, definimos una función para:\n",
    "\n",
    "1. tokenizar el texto de entrada y codificarlo como un vector con los pesos obtenidos por el modelo GPT2\n",
    "2. predecir la siguiente palabra más frecuente\n",
    "3. decodificar el vector como una secuencia de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text, length=50):\n",
    "    \"\"\"Generate automatic Natural Language from the input text\"\"\"\n",
    "    vec_text = tokenizer.encode(text)\n",
    "    my_input, past = torch.tensor([vec_text]), None\n",
    "    \n",
    "    for _ in range(length):\n",
    "        logits, past = model(my_input, past=past)\n",
    "        my_input = torch.multinomial(F.softmax(logits[:, -1]), 1)\n",
    "        vec_text.append(my_input.item())\n",
    "    \n",
    "    return tokenizer.decode(vec_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defino un texto de entrada\n",
    "text = \"The only think we can do to fight climate change is\"\n",
    "\n",
    "# y generamos automáticamente las secuencias más probables\n",
    "print(generate(text, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
