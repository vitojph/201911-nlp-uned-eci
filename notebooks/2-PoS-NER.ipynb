{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vOplmYrqqkhN"
   },
   "source": [
    "# An√°lisis morfosint√°ctico y reconomiento de entidades\n",
    "\n",
    "Se trata de algunas de las tareas m√°s b√°sicas que se realizan a la hora de procesar lenguaje natural:\n",
    "\n",
    "- [an√°lisis morfol√≥gico](https://en.wikipedia.org/wiki/Part-of-speech_tagging) es la tarea que consiste en etiquetar cada palabra de una oraci√≥n de acuerdo con su categor√≠a y sus rasgos gramaticales.   \n",
    "\n",
    "![pos tagging](https://drive.google.com/uc?id=1iczT9qpy68IFxuEtML-ICuV_YU2ttZit)\n",
    "\n",
    "- [an√°lisis sint√°ctico](https://en.wikipedia.org/wiki/Parsing) es la tarea que consiste analizar la estructura de un texto para construir el √°rbol sint√°ctico de e identificar  indicando las funciones que cumplen cada uno de los constituyentes o elementos de la oraci√≥n.\n",
    "\n",
    "![parsig: dependencies](https://drive.google.com/uc?id=1svJNxpFmC1HSUSUX5x-_cqFtfTJEtwcX)\n",
    "\n",
    "- [reconocimiento de entidades nombradas](https://en.wikipedia.org/wiki/Named-entity_recognition) es la tarea que consiste en identificar qu√© palabras de un texto hacen referencia a entidades del mundo y qu√© tipo de entidad es en cada caso. Estos tipos de entidades pueden ser *generalistas* (nombres propios que hacen referencia a personas, organizaciones, lugares, productos, fechas, cantidades, etc) o pueden ser *espec√≠ficas* para un dominio concreto (retail: marcas y modelos de productos, colores, tallas, tejidos; bio m√©dico: ARN, prote√≠nas, tipos de c√©lulas, ADN; banca: direcciones, DNI, referencia catastral, importes, tipo de cargas, nombres de notario, nombre de titular). \n",
    "\n",
    "En ambos casos, estamos asumiendo varias cosas:\n",
    "\n",
    "1. que contamos con lenguaje natural en formato texto, y;\n",
    "2. que somos capaces de segmentar correctamente el texto en oraciones y palabras.\n",
    "\n",
    "Veamos antes un poco m√°s en profundidad qu√© es la tokenizaci√≥n y c√≥mo afecta este paso al resto de tareas.\n",
    "\n",
    "## Tokenizaci√≥n\n",
    "\n",
    "Tokenizaci√≥n es la tarea que consiste en segmentar un texto en *tokens* o unidades m√≠nimas. En muchos casos estos tokens coinciden con nuestra idea intuitiva de *palabra*, pero no tiene por qu√©. Es m√°s una decisi√≥n de dise√±o. \n",
    "\n",
    "Por cierto, ¬øqu√© es una palabra?\n",
    "\n",
    "![words](https://drive.google.com/uc?id=1VhASH6n-mS4SmWtDkrpn9DQ46O9cmUpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"¬´Imagine¬ª es una canci√≥n interpretada por el m√∫sico ingl√©s John Lennon y escrita por Yoko Ono y √©l a principios de 1971. Si bien el sello discogr√°fico Apple Records distribuy√≥ la canci√≥n a partir de octubre de ese a√±o en Estados Unidos, esta no estuvo disponible en Reino Unido hasta 1975. Se trata de una balada de piano del g√©nero soft rock cuya letra hace alusi√≥n a una variedad de conceptos, aunque primordialmente promueve la idea de que ¬´todos somos una naci√≥n, un mundo y una sociedad¬ª. En 1987 se estren√≥ un videoclip para promocionar la canci√≥n, bajo la direcci√≥n de Zbigniew Rybczy≈Ñski.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = texto.split()\n",
    "\n",
    "print(f\"Rompiendo la cadena son split, tenemos {len(tokens)} tokens.\\n\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(texto)\n",
    "\n",
    "print(f\"Rompiendo la cadena son NLTK, tenemos {len(nltk_tokens)} tokens.\\n\")\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si esta celda falla, prueba a instalar antes el corpus cess_esp\n",
    "# nltk.download(\"cess_esp\")\n",
    "\n",
    "from nltk.corpus import cess_esp\n",
    "\n",
    "# cargo el primer documento del corpus segmentado en palabras\n",
    "palabras = cess_esp.words(cess_esp.fileids()[0])[:99]\n",
    "print(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øY qu√© hacemos cuando el texto a tokenizar no solo contiene palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"\"\"Have more than one Twitter account? You can toggle between them on your app! üì± Just follow these handy #TwitterTips: https://t.co/MeqwfQQqah?amp=1 üòÄ\"\"\"\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "t = TweetTokenizer()\n",
    "\n",
    "tweet_tokens = t.tokenize(tweet)\n",
    "print(f\"Usando el TweetTokenizer de NLTK ahora tenemos {len(tweet_tokens)} tokens.\\n\")\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `spaCy`\n",
    "\n",
    "## An√°lisis Morfosint√°ctico con y Reconomiento de entidades con `spaCy`\n",
    "\n",
    "[`spaCy`](http://www.spacy.io/) es una librer√≠a de procesamiento del lenguaje natural, robusta, r√°pida, f√°cil de instalar y utilizar e integrable con [otras librer√≠as de *NLP* y de *deep learning*](https://spacy.io/usage/facts-figures#section-other-libraries). \n",
    "\n",
    "Tiene unos cuantos [modelos ya entrenados en varios idiomas](https://spacy.io/usage/models) y permite realizar las [t√≠picas tareas](https://spacy.io/usage/facts-figures) de segmentaci√≥n por oraciones, tokenizanci√≥n, an√°lisis morfol√≥gico, extracci√≥n de entidades y an√°lisis de opini√≥n.\n",
    "\n",
    "Empezamos cargando un modelo en espa√±ol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNzfLSHAqh6l"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# cargamos el modelo entrenado en espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# si la carga de modelos no funciona, prueba a instalarlos antes\n",
    "#!python -m spacy download es_core_news_md\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNzfLSHAqh6l"
   },
   "outputs": [],
   "source": [
    "# analizamos el texto de entrada\n",
    "doc = nlp(texto)\n",
    "\n",
    "# y podemos iterar sobre las oraciones de doc\n",
    "for n, oracion in enumerate(doc.sents):\n",
    "    print(f\"\\nOraci√≥n {n+1}: {oracion}\\n\")\n",
    "    \n",
    "    # para imprimir la informaci√≥n morfo-sint√°ctica de cada token\n",
    "    for token in oracion:\n",
    "        print(f\"{token.text} tiene lema {token.lemma_}, categor√≠a {token.tag_} y funci√≥n {token.dep_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces es √∫til poder visualizar el √°rbol de dependencias. Podemos hacerlo directamente desde aqu√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1572623653800,
     "user": {
      "displayName": "Victor Peinado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAuZrqdQYMynRZFHuHXa0ZIfJZ9gCiRxyikWdTHLos=s64",
      "userId": "00024393148693047681"
     },
     "user_tz": 420
    },
    "id": "aGPa-oJoq_vS",
    "outputId": "e4bd0d3b-50b5-488e-f798-e0ed6fe91464"
   },
   "source": [
    "El objeto `doc` que hemos creado al analizar el texto de entrada tambi√©n ya ejecutado el reconocimiento de entidades y es capaz de reconocer hasta [4 tipos de entidades: PER, LOC, ORG, MISC](https://spacy.io/api/annotation#ner-wikipedia-scheme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1572623653800,
     "user": {
      "displayName": "Victor Peinado",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAuZrqdQYMynRZFHuHXa0ZIfJZ9gCiRxyikWdTHLos=s64",
      "userId": "00024393148693047681"
     },
     "user_tz": 420
    },
    "id": "aGPa-oJoq_vS",
    "outputId": "e4bd0d3b-50b5-488e-f798-e0ed6fe91464"
   },
   "outputs": [],
   "source": [
    "print(f\"El texto tiene {len(doc.ents)} entidades distintas.\\n\")\n",
    "\n",
    "for entidad in doc.ents:\n",
    "    print(f\"La entidad {entidad} es del tipo {entidad.label_}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con el an√°lisis de dependencias, podemos visualizar las entidades nombradas de manera gr√°fica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øQu√© tal funciona en ingl√©s? Carguemos un modelo probemos con algunos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = \"\"\"After Apple discovered in June that certain MacBook laptops could overheat, posing a fire hazard, the Consumer Product Safety Commission quickly issued a warning, along with information about consumer burns and smoke inhalation. But after Apple learned that its FaceTime video chat app was enabling consumers to listen in on the conversations of people they called ‚Äî even when the recipients did not answer their phones ‚Äî there was no designated federal protection agency to warn Americans or collect reports of privacy invasions.\"\"\"\n",
    "\n",
    "doc_en = nlp_en(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y podemos iterar sobre las oraciones de doc\n",
    "for n, oracion in enumerate(doc_en.sents):\n",
    "    print(f\"\\nOraci√≥n {n+1}: {oracion}\\n\")\n",
    "    \n",
    "    # para imprimir la informaci√≥n morfo-sint√°ctica de cada token\n",
    "    for token in oracion:\n",
    "        print(f\"{token.text} tiene lema {token.lemma_}, categor√≠a {token.tag_} y funci√≥n {token.dep_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depende del tipo de textos con los que problemos, veremos mejores o peores resultados. Debemos tener siempre en cuenta el tipo de lengua con el que modelo ha sido entrenado. Y los nombres de los modelos de `spaCy` nos dan una pista: \n",
    "\n",
    "- [`es_core_news_md`](https://spacy.io/models/es#es_core_news_md) es el modelo mediano entrenado en espa√±ol con datos de noticias (lengua est√°ndar, variante culta).\n",
    "\n",
    "- [`en_core_web_md`](https://spacy.io/models/en#en_core_web_md) es el modelo mediano entrenado en ingl√©s con datos de la web (una mezcla de textos de Wikipedia y medios online, blogs, comentarios de redes sociales y foros, con muestras de lengua culta pero tambi√©n usos coloquiales).\n",
    "\n",
    "Lo que s√≠ funciona menor en ingl√©s es el reconocimiento de entidades, porque [los datos de entrenamiento contienen m√°s tipos de entidad](https://spacy.io/api/annotation#named-entities). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"El texto tiene {len(doc_en.ents)} entidades distintas.\\n\")\n",
    "\n",
    "for entidad in doc_en.ents:\n",
    "    print(f\"La entidad {entidad} es del tipo {entidad.label_}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `flair`\n",
    "\n",
    "La compa√±ia Zalando tiene necesidades de aplicar NLP en distintos √°mbitos y su equipo de investigaci√≥n ha liberado recientemente [`flair`](https://github.com/zalandoresearch/flair), su librer√≠a de NLP.\n",
    "\n",
    "`flair` permite acceder a funcionalidades muy interesantes para procesar lenguaje natural, algunas de ellas muy modernas como:\n",
    "\n",
    "- [etiquetar morfo-sint√°cticamente](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md)\n",
    "- extraer entidades\n",
    "- clasificar autom√°ticamente texto\n",
    "- entrenar tus propios modelos para [construir otros clasificadores](https://towardsdatascience.com/text-classification-with-state-of-the-art-nlp-library-flair-b541d7add21f)\n",
    "- [cargar vectores de palabras en decenas de lenguas](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md)\n",
    "- [usar vectores contextuales como BERT, ELMo](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md)\n",
    "\n",
    "## An√°lisis Morfosint√°ctico con y Reconomiento de entidades con `flair`\n",
    "\n",
    "Para analizar sint√°cticamente un texto, necesitamos cargar un [etiquetador con un modelo concreto de informaci√≥n morfo-sint√°ctica](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md). Por ejemplo, uno en ingl√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# cargamos el analizador multi-idioma\n",
    "tagger = SequenceTagger.load(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Sentence(\"Why are Americans protected from hazardous laptops, fitness trackers and smartphones ‚Äî but not when hazardous apps on our devices expose and exploit our personal information?\")\n",
    "\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flair` tiene una cosa muy interesante, un modelo con informaci√≥n morfol√≥gica multi-idioma. Veamos como funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# cargamos el analizador multi-idioma\n",
    "multi_tagger = SequenceTagger.load(\"pos-multi-fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = Sentence(\"Facebook naci√≥ hace d√©cada y media tras una noche de copas de Mark Zuckerberg. \")\n",
    "multi_tagger.predict(sentence1)\n",
    "# imprimimos el an√°lisis\n",
    "print(sentence1.to_tagged_string(), \"\\n\")\n",
    "\n",
    "sentence2 = Sentence(\"Grand d√©bat national: suivez Emmanuel Macron en direct de Bordeaux. \")\n",
    "multi_tagger.predict(sentence2)\n",
    "# imprimimos el an√°lisis\n",
    "print(sentence2.to_tagged_string(), \"\\n\")\n",
    "\n",
    "sentence3 = Sentence(\"Hier an der Zufahrt zur Startrampe 39A, wo vor 50 Jahren die gigantischen Saturn-Raketen der Apollo-Mondmissionen im Schneckentempo vorbeigefahren sind, prangen nun die blauen Lettern des Raumfahrtunternehmens von Elon Musk an einem Hangar.\")\n",
    "multi_tagger.predict(sentence3)\n",
    "# imprimimos el an√°lisis\n",
    "print(sentence3.to_tagged_string(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar el reconocimiento de entidades, solo necesitamos cargar un modelo preentrenado con informaci√≥n de este tipo. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# cargamos el reconocedor de entidades\n",
    "multi_ner = SequenceTagger.load(\"ner-multi-fast\")\n",
    "\n",
    "multi_ner.predict(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimimos el an√°lisis\n",
    "print(sentence1.to_tagged_string())\n",
    "\n",
    "# iteramos por la entidades\n",
    "for entity in sentence1.get_spans(\"ner\"):\n",
    "    print(entity)\n",
    "    \n",
    "# o imprimimos la estructura de datos con el an√°lisis completo\n",
    "print(sentence1.to_dict(tag_type=\"ner\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el ingl√©s, existe un modelo entrenado con el mismo corpus que `spaCy`, aunque seg√∫n los autores de `flair` ellos consiguen mejores resultados. Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# cargamos el reconocedor de entidades\n",
    "ner = SequenceTagger.load(\"ner-ontonotes-fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence4 = Sentence(\"Behind closed doors, freshman Rep. Alexandria Ocasio-Cortez threatened to put those voting with Republicans ‚Äúon a list‚Äù for a primary challenge in the 2020 election.\")\n",
    "ner.predict(sentence4)\n",
    "\n",
    "# imprimimos el an√°lisis\n",
    "print(sentence4.to_tagged_string())\n",
    "\n",
    "# iteramos por la entidades\n",
    "for entity in sentence4.get_spans(\"ner\"):\n",
    "    print(entity)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2-PoS-NER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
